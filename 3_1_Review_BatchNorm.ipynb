{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3.1 Review_BatchNorm.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/namepen/test_Repo/blob/master/3_1_Review_BatchNorm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "fK6yF0DsCJ2S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Review : Batch normalization\n",
        "\n",
        "대부분의 내용은 [link]()의 내용을 참고하였습니다."
      ]
    },
    {
      "metadata": {
        "id": "vJnxcSaeCJ2X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "목차\n",
        "\n",
        "1. 어떠한 이유로 탄생했는가?, 이득이 뭔가?\n",
        "2. 배치놈이 무엇인가? 어떤 수식인가?\n",
        "3. 테스트 결과\n",
        "4."
      ]
    },
    {
      "metadata": {
        "id": "hcocr3vvCJ2a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##1. Why we need Batch Normalization.\n",
        "\n",
        "일반적인 모델에서, layer을 통과하는 과정은 Input X에 weight W를 곱하고 bias b를 더한 후에 activation function F를 지나게 된다.\n",
        "\n",
        "{math}Y = F(W * X + b){/math}\n",
        "\n",
        "여기서 Sigmoid 함수를 살펴보면 이러한 형태로 되어있는데, Input X의 절대값이 클수록 g'(x)의 값이 0에 가까이 가게된다.(이를 saturate되었다고 한다) 이러한 layer을 깊이 쌓으면 gradient 변화가 거의 없기때문에 학습이 어려워지거나 진행되지 않게된다.\n",
        "\n",
        "Relu의 경우에는 max(x,0)이므로 위의 saturation 문제가 줄어들게 되었지만, 망이 깊어지면 여전히 문제가 발생하게 된다.\n",
        "\n",
        "위 문제를 해결하기 위해 논문에서는 Input X의 분포를 변하지않게 고정하는 것이 긍정적인 영향을 준다고 말한다(기존에는 careful initialization, learning rate settings에 신경써왔다). 또한, 이렇게 input과 output의 분포가 달라지게 되면, layer을 점점 지날수록 초기 데이터의 성질을 반영하지 못한다고 불 수 있다.<br>\n",
        "\n",
        "layer을 통과하면서 Input X의 분포가 변화하는 현상을 Internal Covariate Shift라고 한다. Batch Normalization은 이 현상을 억제하여 학습을 빠르게 진행할 수 있게하고(높은 learning rate로 학습가능), regularization같은 역활을 해서 기존의 Dropout을 대체 할 수 있다."
      ]
    },
    {
      "metadata": {
        "id": "m7xG0RrXJ6vH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "3P_J_4VXCJ2b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "논문에서 Batch Normalization을 사용함으로서 Internal Covariate Shift를 막을 수 있다고 한다. Internal Covariate Shift는 Input X가 layer를 통과하면서 기존의 분포가 변형되는 것을 말한다. 이렇게 분포가 바뀌는 현상이 발생하는 것이 gradient 문제와 연결되며, \n",
        "\n",
        "\n",
        "일반적인 모델에서, layer을 통과하는 과정은 Input X에 weight W를 곱하고 bias b를 더한 후에 activation function F를 지나게 된다.\n",
        "\n",
        "{math}Y = F(W * X + b){/math}\n",
        "\n",
        "여기서 Sigmoid 함수를 살펴보면 이러한 형태로 되어있는데, Input X의 절대값이 클수록 g'(x)의 값이 0에 가까이 가게된다.(이를 saturate되었다고 한다) 이러한 layer을 깊이 쌓으면 gradient 변화가 거의 없기때문에 학습이 어려워지거나 진행되지 않게된다.\n",
        "\n",
        "Relu의 경우에는 max(x,0)이므로 위의 saturation 문제가 줄어들게 되었지만, 망이 깊어지면 여전히 문제가 발생하게 된다.\n",
        "\n",
        "위 문제를 해결하기 위해 논문에서는 Input X의 분포를 변하지않게 고정하는 것이 긍정적인 영향을 준다고 말한다.(기존에는 careful initialization, learning rate settings에 신경써왔다) 또한, 이렇게 input과 output의 분포가 달라지게 되면, layer을 점점 지날수록 초기 데이터의 성질을 반영하지 못한다고 불 수 있다.\n",
        "\n",
        "BN의 가장 큰 효과는 Internal covariate shift를 막는 것, 즉 Input X의 분포를 계속 유지할 수 있게 만드는 것이다.\n",
        "\n",
        "\n",
        "\n",
        "Sigmoid나 tanh을 activation F로 사용하는 경우, 입력의 절대값이 작은 일부 구간을 제외하면 미분값이 0근처로 가게되고(이를 saturated라고 표현한다), 이러한 layer을 깊게 쌓아 모델을 만들게되면 gradient의 변화가 거의 없기때문에 역전파(back-propagation)을 이용한 학습이 어려워거나 되지않는 현상이 발생하게 된다.\n",
        "\n",
        "Relu를 쓰는 경우에는 이 문제를 어느정도 해결할 수 있지만, gradient값이 0이되는 vanishing gradient 문제는 여전히 발생한다. 이러한 문제를 해결하기 위해 careful initialization, learning rate settings를 신경써왔다.\n",
        "\n",
        "위 문제를 해결하기 위해 논문에서는 Input X의 분포를 변하지않게 고정하는 것이 긍정적인 영향을 준다고 말한다. \n",
        "beneficial effect on the gradient flow 에 긍정적인 영향을 준다."
      ]
    },
    {
      "metadata": {
        "id": "DN77eP0bCJ2e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "딥러닝 모델을 깊게 쌓을 때, 발생하는 문제 중 하나가 vanishing/exploding gradient문제이다. (이 문제를 방지하기 위해 Careful Initialization, small learning rate를 사용해왔다.)\n",
        "\n",
        "또한 sigmoid 같은 activation function을 사용하게되면, gradient가 0으로 수렴하게 되어 saturate 되는 현상이 발생한다. layer를 지나면서 이 현상이 중첩되면\n",
        "학습 속도를 감소시켜 학습이 잘 이루어지지 않게된다. Relu에서는 이 saturation 문제를 감소시켰으나, vanishing gradient 문제는 여전히 발생한다. \n",
        "\n",
        "네트워크가 깊어질 수록 작은 변화에도 민감하게 작용하기 때문에, 초기 데이터(X)의 분포를 고정시키는 것이 유리하게 작용한다.\n",
        "\n",
        "그리고 각 layer를 통과하면서 input의 분포(distribution)과 output의 분포가 달라지는 문제도 일어나는데 이를, covariance shift라고 한다.\n",
        "\n",
        "이렇게 input과 output의 분포가 달라지게 되면, layer을 점점 지날수록 초기 데이터의 성질을 반영하지 못한다고 불 수 있다."
      ]
    },
    {
      "metadata": {
        "id": "e4YMZCmsCJ2k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We define 'Internal Covariate Shift' as the change in the\n",
        "distribution of network activations due to the change in\n",
        "network parameters during training. T\n",
        "\n",
        "논문에서 저자는 마지막 알파 베타를 추가하는 이유가 Note that simply normalizing each input of a layer may change what the layer can represent.\n",
        "layer에서 보여주는 것들을 변화시킬 수 있다고 하며, 해결책으로서 넣었다고 주장한다. \n",
        "\n",
        "To address this, we make sure that the transformation inserted in the network\n",
        "can represent the identity transform(원래의 형태로 돌리는 것)\n",
        "\n",
        "이 계수들은 원래 모델의 parameters처럼 학습이 가능하다.\n",
        "\n",
        "Batch Normalization adds\n",
        "only two extra parameters per activation, and in doing so\n",
        "preserves the representation ability of the network\n",
        "\n",
        "The resulting networks can be trained with saturating nonlinearities, are more tolerant to increased training rates, and often do not require Dropout for regularization.\n",
        "\n",
        "장점 : 높은 learning rate를 사용할 수 있다. dropout 같은 regularization technique을 사용하지 않아도 된다.(BN이 regularization과 같은 효과를 내기떄문)\n",
        "\n",
        "The goal of Batch Normalization is to achieve a stable distribution of activation values throughout training, and in our experiments we apply it\n",
        "before the nonlinearity since that is where matching the first and second moments is more likely to result in a stable distribution\n",
        "\n",
        "\n"
      ]
    }
  ]
}